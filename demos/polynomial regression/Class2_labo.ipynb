{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning- regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A real estate agent currently only has Single-Family housed in his portfolio. He wants to expand his business to apartments, but he doesn't have enough experience to give reliable appraisels. Getting the necessary experience would take a lot of time and he doesn't have any colleagues to fall back on. He knows we are following a machine learning course and has a brilliant idea. He give us a data-set with a lot of information on real estate, including the known selling price (tx_price).  He asks us to build a real-estate pricing model for apartmens.\n",
    "\n",
    "We already cleaned this dataset in the first class (the data hasn't been standardized yet). Perform a simple linear regression and polynomial regression to predict the price for apartments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('real_estate_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already cleaned, but not standardized yet. We will take a quick look at the data to get to know the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Take a look at the data\n",
    "1. Look at the dimensions (number of features and observations)\n",
    "2. Look at the first 5 rows\n",
    "3. Look at the different features and there data types\n",
    "    + What do you notice with regard to the datatypes of the one-hot encoded features?\n",
    "    + Fix this\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. First rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have noticed that they are not in the correct data type. Convert them to uint8, using '.astype(np.uint8)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,25:39] = df.iloc[:,25:39]. ...(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Train/test-split and standardisation\n",
    "\n",
    "1. Shuffle your data\n",
    "\n",
    "2. Make a Train/test \n",
    "    - Use random state=123 whenever needed\n",
    "    - Use a test size of 20%\n",
    "    \n",
    "3. Standardize both datasets  \n",
    "    + Make sure you only standardise the numerical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "df_shuffle = df. ...(frac=1, random_state=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train/test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split of feaures and outcomes\n",
    "X = df_shuffle.drop(['...'],1)\n",
    "y = df_shuffle['...']\n",
    "\n",
    "# Perform train/test-split\n",
    "X_train, X_test, y_train, y_test = ...(X, y, test_size=..., random_state=...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler. ...(X_train[num_feat])\n",
    "\n",
    "X_train_stan = X_train.copy()\n",
    "X_test_stan = X_test.copy()\n",
    "\n",
    "X_train_stan[num_feat] = scaler. ...(X_train[...])\n",
    "X_test_stan[...] = ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear regression\n",
    "1. Train a linear regression model, using the standardized data.\n",
    "2. Test the trained model on the train and the test set.\n",
    "    + Predict the price of the appartments\n",
    "    + Calculate the coefficient of determination\n",
    "    + Calculate the Mean Absolute Error (MAE)\n",
    "    + Calculate the Mean Square Error (MSE)\n",
    "    + Would you say this model is overfitted, underfitted or neither?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg= LinearRegression()\n",
    "reg.fit(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Evaluating the model\n",
    "    + make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = reg.predict(...)\n",
    "predictions_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   + Coefficient of determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(reg.score(..., ...))\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative code\n",
    "from sklearn.metrics import r2_score\n",
    "print(...(y_train, predictions_train))\n",
    "print(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ...\n",
    "\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Polynomial regression\n",
    "## 2.1 Quadratic model\n",
    "\n",
    "We will do a quadratic polynomial regression to see if we can improve the reliability of the model. \n",
    "1. Design polynomial features with degree 2\n",
    "    + Don't forget to also transform the test data\n",
    "    + Check the number of features of the new datasets \n",
    "2. Fit a linear regression to the polynomial features\n",
    "    + Use the 'fit_intercept=False'-argument\n",
    "3. Evaluate the train and test-set, using R^2 \n",
    "    +  Would you say this model is overfitted, underfitted or neither?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Design the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = ...(degree=...)\n",
    "X_train_poly = ...\n",
    "X_test_poly = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of features\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "reg_quad = LinearRegression(fit_intercept=...)\n",
    "\n",
    "#Fit the model\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Higher order polynomial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do a cross-validation to find the optimal order for the polynomial.\n",
    "    + Use a pipeline that entails two steps: engineering the polynomial features and fitting the regression\n",
    "    + Let the degree of the polynomial range from 1 to 5.\n",
    "    + Ask python to print out the R^2 for each degree\n",
    "    + What do you expect to happen? Will increasing the degree of the polynomial solve the overfitting or just make it worse?\n",
    "    \n",
    "2. Make a plot of the cross-validation results\n",
    "     + Did your expectation come true?\n",
    "    + Overfitted or underfitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "avg_scores = [None] * 5\n",
    "\n",
    "for i in np.arange(1,6):\n",
    "    \n",
    "    reg_poly = Pipeline(... ,\n",
    "                        ...)\n",
    "    \n",
    "    scores = cross_val_score(...)\n",
    "    \n",
    "    avg_scores[i-1] = scores.mean()\n",
    "    \n",
    "    print(\"Order \"+str(i)+\": avg R^2 = \"+str( avg_scores[i-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.scatter(np.arange(1,6), avg_scores, c='b', label='data')\n",
    "plt.axis('tight')\n",
    "plt.title(\"Cross-validation polynomials\")\n",
    "ax.set_xlabel(\"Order\");\n",
    "ax.set_ylabel(\"CV R^2\");\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "a50344f3b0b7ca814646c9dffd2ebba65c00842c5ce918b7cfb8046ccbc83107"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
